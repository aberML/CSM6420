{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 - Model Comparison using Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV reviews the performance of a set range of parameters on a cross-validation basis. This means only a portion of the training data is reviewed at any one time. When filling in the NA values with the mean value or feature selection, however, we considered the whole set of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we took an inconsistent approach in reviewing only a portion of the data when running GridSearchCV, but the full set of data when filling in missing values. We can avoid this inconsistency by building pipelines and making imputations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas - Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas - Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will leave the NA values in the column Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "age_mean = df['Age'].mean()\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n",
    "mode_embarked = mode(df['Embarked'])[0][0]\n",
    "df['Embarked'] = df['Embarked'].fillna(mode_embarked)\n",
    "\n",
    "df['Gender'] = df['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "pd.get_dummies(df['Embarked'], prefix='Embarked').head(10)\n",
    "df = pd.concat([df, pd.get_dummies(df['Embarked'], prefix='Embarked')], axis=1)\n",
    "\n",
    "df = df.drop(['Sex', 'Embarked'], axis=1)\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols = [cols[1]] + cols[0:1] + cols[2:]\n",
    "\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the NA values in the column Age with a negative value marker -1, as the following bug disallows us from using a missing value marker:\n",
    "\n",
    "https://github.com/scikit-learn/scikit-learn/issues/3044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then review our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn - Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building pipelines\n",
    "We now build pipelines to enable us to first impute the mean value of the column Age on the portion of the training data we are considering, scale the data, select the features, and then assess the performance of our tuning parameters with various learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building a pipeline for random forest model (rf)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "scaler = StandardScaler()\n",
    "anova_filter = SelectKBest(f_regression)\n",
    "imputer = Imputer(strategy='mean', missing_values=-1)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "pipeline_rf = Pipeline([\n",
    "    ('imp', imputer),\n",
    "    ('anova', anova_filter),\n",
    "    ('rf', clf)\n",
    "])\n",
    "\n",
    "# Building a pipeline for SVM classification model (svc)\n",
    "# Normalisation is important to SVM, unlike for decision tree learning. \n",
    "\n",
    "# ANOVA SVM-C\n",
    "clf = SVC(kernel='rbf')\n",
    "pipeline_svm = Pipeline([\n",
    "        ('imp', imputer),\n",
    "        ('scale', scaler), \n",
    "        ('anova', anova_filter), \n",
    "        ('svc', clf)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Grid search using pipelines\n",
    "\n",
    "We now setup parameter grid and run GridSearchCV as before but replacing the classifier with our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline_dict = {'rf': pipeline_rf, 'svm': pipeline_svm} \n",
    "parameter_grid_dict = {}\n",
    "parameter_grid_dict['rf'] = {\n",
    "            'anova__k': [8, 9],\n",
    "            'rf__n_estimators': [100, 1000],\n",
    "            'rf__max_depth': [5, None],\n",
    "        }\n",
    "\n",
    "parameter_grid_dict['svm'] = {\n",
    "            'anova__k': [6, 9],\n",
    "            'svc__C': [0.1, 1, 10],\n",
    "            'svc__gamma': [0.1, 1]\n",
    "        }\n",
    "\n",
    "grid_results = {}\n",
    "for alg in ['rf', 'svm']:\n",
    "    pipeline = pipeline_dict[alg]\n",
    "    parameter_grid = parameter_grid_dict[alg]    \n",
    "    grid_search = GridSearchCV(pipeline, parameter_grid, cv=5, verbose=3)\n",
    "    grid_search.fit(train_data[0::,2::], train_data[0::,0])\n",
    "\n",
    "    sorted(grid_search.grid_scores_, key=lambda x: x.mean_validation_score)\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    grid_results[alg] = grid_search    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Analysis of the grid search results \n",
    "for alg in grid_results:\n",
    "    grid_search = grid_results[alg]\n",
    "    sorted(grid_search.grid_scores_, key=lambda x: x.mean_validation_score)\n",
    "    print 'Best accuracy for %s :' % alg\n",
    "    print(grid_search.best_score_)\n",
    "    print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark:\n",
    "Univariate ANOVA feature selection doesn't seem to improve the performance for both RF and SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation to estimate accuracy and AUC using the selected tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the parameters of the models to the best ones selected from grid search\n",
    "pipeline_rf.set_params(anova__k=9, rf__n_estimators=1000, rf__max_depth=5)\n",
    "pipeline_svm.set_params(anova__k=9, svc__C=1, svc__gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to compare performance of different models, perform CV using the best hyper-parameters selected using grid search based on the previous pipeline analysis. This time we use two runs of 5-fold CV in order to get more reliable estimate of the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold as SKFold\n",
    "\n",
    "X = train_data[0:, 2:]\n",
    "y = train_data[0:, 0]\n",
    "\n",
    "scv1 = SKFold(y=y, n_folds=5, random_state=1234)\n",
    "scv2 = SKFold(y=y, n_folds=5, random_state=5678)\n",
    "\n",
    "def getAccAuc(pipeline, X_train, y_train, X_test, y_test):\n",
    "    \"\"\" To calculate accuracy and auc using different training test sets \n",
    "    given a predefined modelling pipeline.\n",
    "    \"\"\"\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Get prediction on class label from the model\n",
    "    y_prediction = pipeline.predict(X_test)\n",
    "    \n",
    "    # Get probability or decision function output from the model\n",
    "    try:\n",
    "         y_out = pipeline.predict_proba(X_test)[:,1]                     \n",
    "    except AttributeError:\n",
    "         print \"No probability output, use decision function instead!\"\n",
    "         y_out = pipeline.decision_function(X_test)\n",
    "    \n",
    "    acc = np.sum(y_test == y_prediction)*1./len(y_test)\n",
    "    print \"[CV]Prediction accuracy:\", acc\n",
    "    # Compute area under the ROC curve (AUC)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_out)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(\"[CV]           AUC:%s\"%roc_auc)\n",
    "    return([acc, roc_auc])\n",
    "\n",
    "\n",
    "pipeline_dict = {'rf': pipeline_rf, 'svm': pipeline_svm}\n",
    "acc_dict = {}\n",
    "auc_dict = {}\n",
    "for alg in ['rf', 'svm']:  \n",
    "  pipeline = pipeline_dict[alg]\n",
    "  mean_acc = 0.0\n",
    "  mean_auc = 0.0\n",
    "  all_acc = []\n",
    "  all_auc = []\n",
    "  print '--- CV using %s ---' % alg\n",
    "  \n",
    "  for scv in [scv1, scv2]:\n",
    "     for training_set, test_set in scv:\n",
    "        X_train = X[training_set]\n",
    "        y_train = y[training_set]\n",
    "        X_test = X[test_set]\n",
    "        y_test = y[test_set]\n",
    "\n",
    "        [acc, roc_auc] = getAccAuc(pipeline, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "        all_acc.append(acc)\n",
    "        all_auc.append(roc_auc)\n",
    "        acc_dict[alg] = all_acc\n",
    "        auc_dict[alg] = all_auc\n",
    "    \n",
    "  all_acc=np.asarray(all_acc)\n",
    "  all_auc=np.asarray(all_auc)\n",
    "  acc_dict[alg] = all_acc\n",
    "  auc_dict[alg] = all_auc\n",
    "\n",
    "  # print 95% C.I. for both accuracy and AUC based on CV\n",
    "  print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (all_acc.mean(), all_acc.std() * 1.96))\n",
    "  print(\"Mean AUC: %0.3f (+/- %0.3f)\" % (all_auc.mean(), all_auc.std() * 1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Comparison of model performance from CV\n",
    "We now can start to compare the CV performance for the two type of models: rf and svm using boxplots and one sample (student) t-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "for alg in acc_dict:  \n",
    "    all_acc = np.array(acc_dict[alg])\n",
    "    all_auc = np.array(auc_dict[alg])\n",
    "    print \"===\", alg, \"=== \"\n",
    "    print \"95% C.I. for both accuracy and AUC based on CV for \"\n",
    "    print(all_acc)    \n",
    "    print(\"Mean Accuracy: %0.3f (+/- %0.3f)\" % (all_acc.mean(), all_acc.std() * 1.96))\n",
    "    print(all_auc)\n",
    "    print(\"Mean AUC: %0.3f (+/- %0.3f)\" % (all_auc.mean(), all_auc.std() * 1.96))\n",
    "\n",
    "dfacc = pd.DataFrame(np.c_[acc_dict['rf'], acc_dict['svm']], columns=['rf','svm'])    \n",
    "dfacc.plot(kind='box', title='Accuracy from 5-fold CV')\n",
    "dfauc = pd.DataFrame(np.c_[auc_dict['rf'], auc_dict['svm']], columns=['rf','svm'])    \n",
    "dfauc.plot(kind='box', title='AUC from 5-fold CV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_acc = np.array(acc_dict['rf']) - np.array(acc_dict['svm'])\n",
    "diff_auc = np.array(auc_dict['rf']) - np.array(auc_dict['svm'])\n",
    "\n",
    "print 'Test the hypothesis that the mean difference in accuracy between rf and svm is not significantly different from 0:'\n",
    "print(diff_acc)\n",
    "print ' Mean difference = %0.3f' % diff_acc.mean()\n",
    "print(stats.ttest_1samp(diff_acc, popmean=0))\n",
    "print 'Test the hypothesis that the mean difference in AUC between rf and svm is not significantly different from 0:'\n",
    "print(diff_auc)\n",
    "print ' Mean difference = %0.3f' % diff_auc.mean()\n",
    "print(stats.ttest_1samp(diff_auc, popmean=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "1. The mean difference in accuracy and AUC indicates that rf performs slightly better than SVM in auc. The variances in accuracy and AUC for RF are slightly higher than for SVM. And the 95% C.I. of Accuracy for the RF and for SVM indeed overlap, thus the accuracy for RF is not significantly different from SVM (at significance level of 0.05). The same applies to AUC. \n",
    "\n",
    "2. The p-value for the one-sample t-test on difference in accuracy is 0.72 >0.05, which reconfirms that we can't reject the null hypothesis that there is no significant difference in accuracy between RF and SVM. \n",
    "\n",
    "3. Whilst the p-value for the t-test on difference in AUC is 0.22, thus we can't reject the null hypothesis that there is no significant difference in AUC between RF and SVM. \n",
    "\n",
    "More details on how to perform t-tests using scipy, please see: \n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sikit-learn - Model training\n",
    "Now that we've determined the model we are going to use and the desired values for our tuning parameters, we can fill in the -1 values in the column Age with the mean and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].map(lambda x: age_mean if x == -1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 1000, max_depth=5)\n",
    "model = model.fit(train_data[0:,2:],train_data[0:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn - Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "df_test = df_test.drop(['Name', 'Ticket', 'Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill in the NA values in test data with the mean, since there is no analogous problem of snooping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test['Age'] = df_test['Age'].fillna(age_mean)\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fare_means = df.pivot_table('Fare', index='Pclass', aggfunc='mean')\n",
    "df_test['Fare'] = df_test[['Fare', 'Pclass']].apply(lambda x:\n",
    "                            fare_means[x['Pclass']] if pd.isnull(x['Fare'])\n",
    "                            else x['Fare'], axis=1)\n",
    "\n",
    "df_test['Gender'] = df_test['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "df_test = pd.concat([df_test, pd.get_dummies(df_test['Embarked'], prefix='Embarked')],\n",
    "                axis=1)\n",
    "\n",
    "df_test = df_test.drop(['Sex', 'Embarked'], axis=1)\n",
    "\n",
    "test_data = df_test.values\n",
    "\n",
    "output = model.predict(test_data[0::,1::])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas - Preparing for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = np.c_[test_data[:,0].astype(int), output.astype(int)]\n",
    "\n",
    "df_result = pd.DataFrame(result[:,0:2], columns=['PassengerId', 'Survived'])\n",
    "df_result.to_csv('../results/titanic_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking verions\n",
    "\n",
    "Finally, checking the versions of python and relevant libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "print 'Python: ', sys.version_info\n",
    "print 'Pandas: ', pd.__version__\n",
    "print 'Sklearn: ', sklearn.__version__\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
